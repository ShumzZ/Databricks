{"cells":[{"cell_type":"markdown","source":["#### For better formatting and visualization, please check out the project/ notebook at:<br>\n[Databricks notebook link](https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/6227236730275197/3490305292750161/5214049462728440/latest.html)\n\n### Inference pipeline deployment (to AWS SageMaker)\n\n- General description:\n  Partially following examples from Databricks documentations [notebook1](https://docs.databricks.com/applications/mlflow/tracking-examples.html#train-a-pyspark-model-and-save-in-mleap-format) and [notebook2](https://docs.databricks.com/applications/mlflow/mleap-model-deployment-on-sagemaker.html), and AWS SageMaker documentations [notebook3](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/advanced_functionality/inference_pipeline_sparkml_blazingtext_dbpedia/inference_pipeline_sparkml_blazingtext_dbpedia.ipynb) and [notebook4](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/advanced_functionality/inference_pipeline_sparkml_xgboost_car_evaluation/inference_pipeline_sparkml_xgboost_car_evaluation.ipynb), the goal of this project is to demonstrate the main steps to deploy a trained inference pipeline to AWS SageMaker:\n  - process features and train models(in particular a text classification model) using `Spark`\n  - log / save trained model and construct inference pipeline (pre-processing, prediction, post-processing) in `mleap`- compatible format using `mlflow`\n  - deploy to AWS SageMaker for real-time prediction requests.\n  \n- Data description:\n  - The modeling trainning section uses the [20 Newsgroups dataset](http://kdd.ics.uci.edu/databases/20newsgroups/20newsgroups.html) which consists of articles from 20 Usenet newsgroups, for details see the link above. Simply put, there are 20000 messages in total, taken from 20 newsgroups, which can be interpreted as 'topics', or 'labels' (such as `rec.autos`, `sci.electronics`, `talk.politics.guns`, etc.), each observation consists of two columns: `text` and `topic`.\n  - Note: If using Databricks, this dataset is **pre-loaded** into Databricks in parquet format under path `file:/dbfs/databricks-datasets/news20.binary/data-001/training`; otherwise, the data is accessible at [this link](http://kdd.ics.uci.edu/databases/20newsgroups/20_newsgroups.tar.gz) (17.3M; 61.6M uncompressed)\n\n- Project outline:\n  - PART 0: set up environment (Databricks, AWS, Docker, etc.)\n  - PART I: Feature engineering, model training and logging\n  - PART II: Deploy to AWS SageMaker (from scratch using `boto3` and concise implementation using `mlflow`)\n      \n- Framework / Language/ Libraries/ Others:\n  - Spark APIS (datafram based ML API `spark.ml`) \n  - Python\n  - PySpark, mleap, mlflow, boto3, sagemaker\n  - Basic Linux knowledge and shell scripts\n\n#### PS: all codes tested on runtime version: 6.4ML (includes Apache Spark 2.4.5, Scala 2.11)\n\n#### For better formatting and visualization, please check out the project/ notebook at:<br>\n[Databricks notebook link](https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/6227236730275197/3490305292750161/5214049462728440/latest.html)"],"metadata":{}},{"cell_type":"markdown","source":["### PART 0: Set up environment (Databricks, AWS, Docker, etc.)"],"metadata":{}},{"cell_type":"markdown","source":["#### Databricks\n- Launch a Python3 cluster with Databricks Runtime 6.4 (DBR64) with:\n   - instance type: m4.large or any other instance type, igonre for Databricks community version\n   - ~~spark config: `spark.databricks.mlflow.trackMLlib.enabled true` # helps to automatically track parameters when using `pyspark.ml.tuning.CrossValidator()`~~<br> no longer necessary\n   - additional libraries installed on the cluster:\n     - PyPI:\n       - `boto3==1.9.215`\n       - `mleap==0.8.1` As or Mar. 2020, version==0.15.0\n       - `mlflow[extra]` see [MLflow doc](https://www.mlflow.org/docs/latest/tutorials-and-examples/tutorial.html#what-you-ll-need)\n       - `sagemaker==1.42.4`\n     - Maven:\n       - `ml.combust.mleap:mleap-spark_2.11:0.13.0`<br>As of Mar. 2020, '...SageMaker SparkML Serving is powered by MLeap 0.13.0 and it is tested with Spark major version - 2.3' [ref](https://github.com/aws/sagemaker-sparkml-serving-container)\n       - `ml.dmlc:xgboost4j-spark:0.90` required, even if no xgboost algorithm is used\n\n#### AWS       \n- AWS CLI version 2 installation and configuration, for details see [official doc](https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2-mac.html)\n- Configure an AWS IAM role with full access to AWS SageMaker, for details see [Databricks Guide](https://docs.databricks.com/administration-guide/cloud-configurations/aws/sagemaker.html)\n\n#### Docker\n- After AWS CLI is configured, autherize the ***Docker*** to access AWS ECR, details see [official doc](https://docs.aws.amazon.com/AmazonECR/latest/userguide/Registries.html)<br>\n`$ aws ecr get-login-password | docker login --username AWS --password-stdin [aws_account_id].dkr.ecr.[us-west-2].amazonaws.com`\n\n#### Boto3 (optional; but required if using Community Version of Databricks)\nbefore moving on, if the following code is not run on Databricks, or if using a Databricks Community Version, an AWS configuration for boto3 is required, for detals see [Doc](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/configuration.html#guide-configuration)\n\n- Configure AWS on databricks community edition, or non-databricks environment. Run this only ONCE\ndetails see [how to set environment variable](https://forums.databricks.com/questions/11116/how-to-set-an-environment-variable.html)\n\n>`dbutils.fs.put(\"dbfs:/databricks/init/init.bash\", \"\"\" #!/bin/bash`\n`sudo echo export AWS_ACCESS_KEY_ID='xxxxxxxxxxxxxxxx' >> /databricks/spark/conf/spark-env.sh`<br>\n`sudo echo export AWS_SECRET_ACCESS_KEY='xxxxxxxxxxxxxxx' >> /databricks/spark/conf/spark-env.sh`<br>\n`\"\"\", True)`\n\nsubstitute `XXXXXX` with *`AWS_ACCESS_KEY_ID`* and *`AWS_SECRET_ACCESS_KEY`* respectively\n\n###### **Alternatively: set ENV variable by:**\n>`os.environ['AWS_ACCESS_KEY_ID'] = xxxxxxxxxxxxxxxx`<br>\n`os.environ['AWS_SECRET_ACCESS_KEY'] = xxxxxxxxxxxxxxxx`\n\n###### **Check if set successfully:**\n>`%sh less /databricks/spark/conf/spark-env.sh`"],"metadata":{}},{"cell_type":"markdown","source":["#### Import necessary libraries"],"metadata":{}},{"cell_type":"code","source":["# feature processing and modeling\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import DecisionTreeClassifier\nfrom pyspark.ml.feature import StringIndexer, IndexToString, Tokenizer, HashingTF\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n\n# model logging\nimport mlflow         # 1.6.0\nimport mlflow.mleap\nimport mlflow.spark\nimport mleap.pyspark\nfrom mleap.pyspark.spark_support import SimpleSparkSerializer\n\n# model deployment\nimport boto3          # 1.9.162\nimport time\nimport sagemaker      # 1.50.17\nimport mlflow.sagemaker as mfs\nimport json\n\n# os related\nimport os\nimport shutil"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"markdown","source":["### PART I: Feature engineering, model training and logging"],"metadata":{}},{"cell_type":"markdown","source":["#### load training data"],"metadata":{}},{"cell_type":"code","source":["# ============================================================\n# Load training data\ndf = spark.read.parquet(\"/databricks-datasets/news20.binary/data-001/training\").select(\"text\", \"topic\")\ntrain, test_with_label = df.randomSplit([0.8, 0.2], 42)\ntrain.cache().count()\ntest_with_label.cache().count()\n\n# ============================================================\n# to get a sense of what the data look like:\n# sample = train.head(1)[0]\n# print(f'text: \\t{sample[0]} \\n\\ntopic: \\t{sample[1]}')\n\n# if in Databricks, simply do \ndisplay(train.head(1))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>text</th><th>topic</th></tr></thead><tbody><tr><td> cs.utexas.edu!geraldo.cc.utexas.edu!portal.austin.ibm.com!awdprime.austin.ibm.com!karner Subject: Re: Islamic marriage? From: karner@austin.ibm.com (F. Karner)  <C4qAv2.24wG@austin.ibm.com> <1993Apr2.103237.4627@Cadence.COM> Organization: IBM Advanced Workstation Division Originator: frank@karner.austin.ibm.com Lines: 50   In article <1993Apr2.103237.4627@Cadence.COM>, mas@Cadence.COM (Masud Khan) writes: > In article <C4qAv2.24wG@austin.ibm.com> karner@austin.ibm.com (F. Karner) writes: > > > >Okay.  So you want me to name names?  There are obviously no official > >records of these pseudo-marriages because they are performed for > >convenience.  What happens typically is that the woman is willing to move > >in with her lover without any scruples or legal contracts to speak of.  > >The man is merely utilizing a loophole by entering into a temporary > >religious \"marriage\" contract in order to have sex.  Nobody complains, > >nobody cares, nobody needs to know. > > > >Perhaps you should alert your imam.  It could be that this practice is > >far more widespread than you may think.  Or maybe it takes 4 muslim men > >to witness the penetration to decide if the practice exists! > >--  > > >  > Again you astound me with the level of ignorance you display, Muslims > are NOT allowed to enter temporary marriages, got that? There is > no evidence for it it an outlawed practise so get your facts  > straight buddy. Give me references for it or just tell everyone you > were lying. It is not a widespread as you may think (fantasise) in > fact contrary to your fantasies it is not practised at all amongst > Muslims.  First of all, I'm not your buddy!  Second, read what I wrote.  I'm not talking about what muslims are ALLOWED to do, merely what *SOME* practice.  They consider themselves as muslim as you, so don't retort with the old and tired \"they MUST NOT BE TRUE MUSLIMS\" bullshit.  If I gave you the names what will you do with this information?  Is a fatwa going to be leashed out against the perpetrators?  Do you honestly think that someone who did it would voluntarily come forward and confess?  With the kind of extremism shown by your co-religionaries?  Fat chance.  At any rate, there can be no conclusive \"proof\" by the very nature of the act.  Perhaps people that indulge in this practice agree with you in theory, but hope that Allah will forgive them in the end.  I think it's rather arrogant of you to pretend to speak for all muslims in this regard.  Also, kind of silly.  Are you insinuating that because the Koranic law forbids it, there are no criminals in muslim countries?   This is as far as I care to go on this subject.  The weakness of your arguments are for all netters to see.  Over and out... --            DISCLAIMER: The opinions expressed in this posting are mine             solely and do not represent my employer in any way.        F. A. Karner AIX Technical Support | karner@austin.vnet.ibm.com </td><td>alt.atheism</td></tr></tbody></table></div>"]}}],"execution_count":8},{"cell_type":"markdown","source":["#### Pre-processing and post-processing"],"metadata":{}},{"cell_type":"markdown","source":["##### Important note: \nIt is crutial to ensure the **consistency** in data processing between the feature-processing + model-training phase and model-prediction phase; in particular, if the target variable needs to be pre-processed (e.g., apply `spark.ml.feature.labelIndexer` to non-numeric target variable in *multi-classification*), then this process should be ***left out of the pipeline***. Otherwise, when querying the endpoint, a 'dummy' column needs to be added to the payload.\n\nAn [AWS blog](https://aws.amazon.com/blogs/machine-learning/ensure-consistency-in-data-processing-code-between-training-and-inference-in-amazon-sagemaker/) also mentioned a similar situation."],"metadata":{}},{"cell_type":"markdown","source":["##### Target variable and post-processing pipeline\nNote I applied `StringIndexer` on the target variable and I would like to 'reverse' this process in the post-processing by `IndexToString`. However since the 'reverse' happens to the predicted column as opposed to the original indexed column (as is the case in feature engineering), the `IndexToString` cannot infer the label mapping from column metadata. Therefore if I do not specify the label-index mapping, an error will raise: <br>`\"Java.lang.ClassCastException: org.apache.spark.ml.attribute.UnresolvedAttribute$ cannot be cast to org.apache.spark.ml.attribute.NominalAttribute\"`.\n\nTo fix this, notice in `StringIndexer` the default stringOrderType is `'frequencyDesc'` (among `'frequencyAsc'`, `'alphabetDesc'`, `'alphabetAsc'`), I can then extract the label mapping either from fitted model, or according to `stringOrderType`, and pass it into `IndexToString`"],"metadata":{}},{"cell_type":"code","source":["# ============================================================\n# process the target variable \nlabelIndexer = StringIndexer(inputCol=\"topic\", outputCol=\"label\", handleInvalid=\"keep\")              # stringOrderType==\"frequencyDesc\"\nindexed = labelIndexer.fit(train)\ntrain = indexed.transform(train)\n\n# ============================================================\n# Explicitly pass in label mapping to IndexToString\n# 1) OPT 1, extract lable-index mapping from fitted model\nlabel_mapping = indexed.labels\n\n# 2) OPT 2, assuming stringOrderType==\"frequencyDesc\":\n# label_mapping = [row.topic for row in train.select('topic').groupby('topic').count().sort('count', ascending=False).collect()]\n\n# ============================================================\n\nlabel_mapping += ['other']         # deal with unseen labels in the test data, in case we specify handleInvalid=='keep' in StringIndexer\npostProcessor = IndexToString(inputCol=\"prediction\", outputCol=\"orignal_topic\", labels = label_mapping)  \n\n# ============================================================\n# Define preprocessing pipeline, construct a Pipeline object using the defined components, note to exclude target variable transformer\n\ntokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\nhashingTF = HashingTF(inputCol=\"words\", outputCol=\"features\")\ndt = DecisionTreeClassifier()\n\npipeline = Pipeline(stages=[tokenizer, hashingTF, dt])\n\n# ============================================================\n# Train and Fine-tune the model, for simplicity purposes, only tune the 'numFeatures' parameter in HashingTF() vectorizer\nparamGrid = (ParamGridBuilder()\n             .addGrid(hashingTF.numFeatures, [50])\n             .build()\n            )\ncv = CrossValidator(estimator=pipeline, \n                    evaluator=MulticlassClassificationEvaluator(), \n                    estimatorParamMaps=paramGrid\n                   )"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":12},{"cell_type":"markdown","source":["#### model training (including Cross-Validation) and logging"],"metadata":{}},{"cell_type":"code","source":["# ============================================================\n# use Mlflow to log the model with optimal parameters into Mleap-compatible format\n\nwith mlflow.start_run() as run:\n  cvmodel = cv.fit(train)\n  \n  # extract the optimal Pipeline model to be logged\n  fitted_model = cvmodel.bestModel\n  artifact_path = \"model_dbr64\"\n  mlflow.mleap.log_model(spark_model=fitted_model\n                         , sample_input=test_with_label.drop('topic')\n                         , artifact_path=artifact_path\n                        ) \n  # extract experiment_id and model_id\n  # for other attributes, see: https://www.mlflow.org/docs/latest/python_api/mlflow.entities.html#mlflow.entities.RunInfo\n  experiment_id = run.info.experiment_id    # recorde the experiment_id for later deployment\n  run_id = run.info.run_uuid                # recorde the run_id for later deployment\n\nprint(f'experiment_id is {experiment_id}')\nprint(f'run_id is {run_id}')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">MLlib will automatically track trials in MLflow. After your tuning fit() call has completed, view the MLflow UI to see logged runs.\nexperiment_id is 3490305292750161\nrun_id is f327eb341ebb45f9ade72f8bb7b92232\n</div>"]}}],"execution_count":14},{"cell_type":"markdown","source":["### PART II: Deploy to AWS SageMaker"],"metadata":{}},{"cell_type":"markdown","source":["##### Test locally (local machine, or AWS EC2)"],"metadata":{}},{"cell_type":"markdown","source":["- setup proper AWS authorization and install Databricks CLI\n  - pip install mlflow==1.6.0 locally \n  - install docker locally and start docker service\n  - locally build a docker image and push it to AWS ECR:\n    - `$ mlflow sagemaker build-and-push-container`\n  - copy the logged model to a local directory:\n    - use Databricks CLI: `$ databricks fs cp -r </databricks/path/to/logged/test_model/dbr63> </local/path/to/test_model_dbr64/>`\n    - if using community version, CLI will not be avaible, instead:\n      - `%sh`<br>\n        `tar -zcvf ../../dbfs/FileStore/d.tar.gz ../../dbfs/databricks/mlflow/[path]/[to]/artifacts/model_dbr64/`<br>\n        for path to model artifects, click 'Runs' at the top right corner and inspect the saved runs\n      - the file can then be accessed by web browser at: `https://community.cloud.databricks.com/files/d.tar.gz?o=xxxxxxxxxx` where xxxxxxxxxx is the number after `?o=` in your databricks URL, for details, see [official doc](https://docs.databricks.com/data/filestore.html)\n  - run local test:\n    - `$ mlflow sagemaker run-local -m </local/path/to/test_model_dbr64/>`"],"metadata":{}},{"cell_type":"markdown","source":["##### Deploy remotely (to SageMaker)"],"metadata":{}},{"cell_type":"markdown","source":["***Deploy***: from here there are two ways to deploy the model, \n- 1) use `boto3` API to deploy model from scratch (create model, endpoint configuration, and endpoint in sagemaker); \n- 2) use `mlflow.sagemaker` for easier deployment. (for `mlflow.sagemaker.deploy()` soure code, see [here](https://github.com/mlflow/mlflow/blob/58be9c01b587344b953965492e4d6f8aa5476482/mlflow/sagemaker/__init__.py#L151))\n\nThe mainly difference is that with `boto3` we can have fine control over the deployment. For instance, if we have post-processing pipeline in addition to pre-processing pipeline and model, we can create a 'model' in sagemaker via `boto3` that integrate all three pieces together. Otherwise if we only have pre-processing and model, it is much easier to just take advantage of mlflow's sagemaker since it does all the dirty work under the hood."],"metadata":{}},{"cell_type":"code","source":["experiment_id = '3490305292750161'\nrun_id = 'f327eb341ebb45f9ade72f8bb7b92232'\nartifact_path = \"model_dbr64\"\nbucket_name = 'databricks-mlflow-sagemaker'\njob_prefix = 'mlflow-deploy'"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":20},{"cell_type":"code","source":["# ============================================================\n# define and extract info required by deployment\n\nregion = \"us-west-2\" # region of AWS account\n\nboto_session = boto3.Session(region_name=region)\nsess = sagemaker.Session(boto_session=boto_session)\n                         \nsagemaker_session = sess.boto_session.client('sagemaker')\naws_id = boto3.client('sts', region_name=region).get_caller_identity()['Account'] \n# account id of the AWS account associated with Databricks account OR AWS authentication(see setup in PART0); otherwise, substitute with AWS account where the model will be deployed under\n\narn = \"arn:aws:iam::\" + aws_id + \":role/sagemaker_full_access\"   # change 'sagemaker_full_access' according to your own AWS IAM role set-up\n\nmodel_uri = \"/dbfs/databricks/mlflow/\" + experiment_id + \"/\" + run_id +\"/artifacts/\" + artifact_path # this is databricks specific, use local path if needed\nimage_url = aws_id + \".dkr.ecr.\" + region + \".amazonaws.com/mlflow-pyfunc:1.6.0\" \n# change the image tag (':1.6.0' part) according to the mlflow version used\n\nos.environ[\"SAGEMAKER_DEPLOY_IMG_URL\"] = image_url"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":21},{"cell_type":"markdown","source":["###### Deploy from scratch using `boto3`"],"metadata":{}},{"cell_type":"code","source":["# ============================================================\n# Save model(inlcuding pre-processor)\n# model artifactes logged by mlflow:\nmlflow_model_path = f'/dbfs/databricks/mlflow/{experiment_id}/{run_id}/artifacts/model_dbr64/'\n\nimport tarfile\nimport zipfile\n# with mlflow logging the model while training, I can directly create the .tar.gz file with the logged model artifects\nwith tarfile.open(\"/databricks/driver/model.tar.gz\", \"w:gz\") as tar:\n    tar.add(f'{mlflow_model_path}mleap/model/bundle.json', arcname='bundle.json')\n    tar.add(f'{mlflow_model_path}mleap/model/root', arcname='root')\n\n# ============================================================\n# Save post-processor\n# for post-processor, a SparkML transformer, I need to serialize and repacked it \nSimpleSparkSerializer().serializeToBundle(postProcessor, \"jar:file:/databricks/driver/postprocess.zip\", postProcessor.transform(fitted_model.transform(train)))\n\nwith zipfile.ZipFile(\"/databricks/driver/postprocess.zip\") as zf:\n    zf.extractall(\"/databricks/driver/postprocess\")\n\n# Writing back the content as a .tar.gz file, since SageMaker only accepts .tar.gz file\nwith tarfile.open(\"/databricks/driver/postprocess.tar.gz\", \"w:gz\") as tar:\n    tar.add(\"/databricks/driver/postprocess/bundle.json\", arcname='bundle.json')\n    tar.add(\"/databricks/driver/postprocess/root\", arcname='root')\n\n# ============================================================\n# upload model and post-processor to s3 \ns3 = boto3.resource('s3')\nbucket_name = 'databricks-mlflow-sagemaker'\njob_prefix = 'mlflow-deploy'\n\nfile_name_1 = job_prefix + '/' + 'model.tar.gz'\ns3.Bucket(bucket_name).upload_file('/databricks/driver/model.tar.gz', file_name_1)\nfile_name_2 = job_prefix + '/' + 'postprocess.tar.gz'\ns3.Bucket(bucket_name).upload_file('/databricks/driver/postprocess.tar.gz', file_name_2)\n\n# ============================================================\n# clean up, \n# OR, `%sh ls /databricks/driver/` this folder will be purged every time the cluster is restarted, and thus could be used as a tmp folder, no need for clean-up\n\n# os.remove('/databricks/driver/model.zip')\nos.remove('/databricks/driver/postprocess.zip')\nos.remove('/databricks/driver/model.tar.gz')\nos.remove('/databricks/driver/postprocess.tar.gz')\n# shutil.rmtree('/databricks/driver/model')\nshutil.rmtree('/databricks/driver/postprocess')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":23},{"cell_type":"code","source":["# ============================================================\n# define input and output schema for both model (including pre-processing) and post-processing,\n# for detailed format for the schema, see https://github.com/aws/sagemaker-sparkml-serving-container\n\nimport json\nmodel_schema = {\"input\":[{\"type\":\"string\",\"name\":\"text\"}]\n                ,\"output\":{\"type\":\"double\",\"name\":\"prediction\"}}\nmodel_schema_json = json.dumps(model_schema)\n\npost_processor_schema = {\"input\": [{\"type\": \"double\", \"name\": \"prediction\"}]\n                         , \"output\": {\"type\": \"string\", \"name\": \"orignal_topic\"}}\npost_processor_schema_json = json.dumps(post_processor_schema)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":24},{"cell_type":"code","source":["from botocore.exceptions import ClientError\nsgm = boto3.client('sagemaker',region_name='us-west-2')\n\ntimestamp_prefix = time.strftime(\"%Y%m%d-%H%M\", time.gmtime())\napp_name = \"news20-boto3-\"+timestamp_prefix\n\n# all SparkML serving container image locations are published at: https://github.com/aws/sagemaker-sparkml-serving-container\nsparkml_images = {\n    'us-west-1': '746614075791.dkr.ecr.us-west-1.amazonaws.com/sagemaker-sparkml-serving:2.2',\n    'us-west-2': '246618743249.dkr.ecr.us-west-2.amazonaws.com/sagemaker-sparkml-serving:2.2',\n    'us-east-1': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-sparkml-serving:2.2',\n    'us-east-2': '257758044811.dkr.ecr.us-east-2.amazonaws.com/sagemaker-sparkml-serving:2.2',\n    'ap-northeast-1': '354813040037.dkr.ecr.ap-northeast-1.amazonaws.com/sagemaker-sparkml-serving:2.2',\n    'ap-northeast-2': '366743142698.dkr.ecr.ap-northeast-2.amazonaws.com/sagemaker-sparkml-serving:2.2',\n    'ap-southeast-1': '121021644041.dkr.ecr.ap-southeast-1.amazonaws.com/sagemaker-sparkml-serving:2.2',\n    'ap-southeast-2': '783357654285.dkr.ecr.ap-southeast-2.amazonaws.com/sagemaker-sparkml-serving:2.2',\n    'ap-south-1': '720646828776.dkr.ecr.ap-south-1.amazonaws.com/sagemaker-sparkml-serving:2.2',\n    'eu-west-1': '141502667606.dkr.ecr.eu-west-1.amazonaws.com/sagemaker-sparkml-serving:2.2',\n    'eu-west-2': '764974769150.dkr.ecr.eu-west-2.amazonaws.com/sagemaker-sparkml-serving:2.2',\n    'eu-central-1': '492215442770.dkr.ecr.eu-central-1.amazonaws.com/sagemaker-sparkml-serving:2.2',\n    'ca-central-1': '341280168497.dkr.ecr.ca-central-1.amazonaws.com/sagemaker-sparkml-serving:2.2',\n    'us-gov-west-1': '414596584902.dkr.ecr.us-gov-west-1.amazonaws.com/sagemaker-sparkml-serving:2.2'\n}\n\n# ============================================================\n# Create SageMaker models (separate models in separate containers)\ntry:\n    sparkml_image = sparkml_images['us-west-2']\n\n    response = sgm.create_model(\n        ModelName=app_name,\n        Containers=[\n            {\n                'Image': sparkml_image\n                , 'ModelDataUrl': f's3://{bucket_name}/{job_prefix}/model.tar.gz'\n                , 'Environment': {\n                    'SAGEMAKER_SPARKML_SCHEMA': model_schema_json\n                }\n            },\n            {\n                'Image': sparkml_image\n                , 'ModelDataUrl': f's3://{bucket_name}/{job_prefix}/postprocess.tar.gz'\n                , 'Environment': {\n                    'SAGEMAKER_SPARKML_SCHEMA': post_processor_schema_json\n                }\n\n            }\n        ],\n        ExecutionRoleArn=arn\n    )\n\n    print(f'{response}\\n')\n    \nexcept ClientError:\n    print('Model already exists, continuing...')\n\n# ============================================================\n# Create SageMaker endpoint configuration\ntry:\n    response = sgm.create_endpoint_config(\n        EndpointConfigName=app_name,\n        ProductionVariants=[\n            {\n                'VariantName': 'DefaultVariant',\n                'ModelName': 'news20-boto3',\n                'InitialInstanceCount': 1,\n                'InstanceType': 'ml.m5.large',\n            },\n        ],\n    )\n    print(f'{response}\\n')\n\nexcept ClientError:\n    print('Endpoint config already exists, continuing...')\n\n# ============================================================\n# Create SageMaker endpoint\ntry:\n    response = sgm.create_endpoint(\n        EndpointName=app_name,\n        EndpointConfigName='news20-boto3',\n    )\n#     # if there is need to update model or configuration or anything:\n#     # https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.update_endpoint\n#     response = sgm.update_endpoint(\n#         EndpointName='test-mlflow',\n#         EndpointConfigName='news20-csv',\n#         RetainAllVariantProperties=True,\n#     )\n    print(f'{response}\\n')\n\nexcept ClientError:\n    print(\"Endpoint already exists, continuing...\")\n    \n# ============================================================\n# Monitor the status until completed\nwhile True:\n    endpoint_status = sgm.describe_endpoint(EndpointName=app_name)['EndpointStatus']\n    print(endpoint_status)\n    if endpoint_status in ('OutOfService','InService','Failed'):\n        break\n    time.sleep(60)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">{&#39;ModelArn&#39;: &#39;arn:aws:sagemaker:us-west-2:360823276916:model/news20-boto3&#39;, &#39;ResponseMetadata&#39;: {&#39;RequestId&#39;: &#39;60d40c6c-5a67-436a-828c-72af1b04a689&#39;, &#39;HTTPStatusCode&#39;: 200, &#39;HTTPHeaders&#39;: {&#39;x-amzn-requestid&#39;: &#39;60d40c6c-5a67-436a-828c-72af1b04a689&#39;, &#39;content-type&#39;: &#39;application/x-amz-json-1.1&#39;, &#39;content-length&#39;: &#39;74&#39;, &#39;date&#39;: &#39;Fri, 03 Apr 2020 06:36:19 GMT&#39;}, &#39;RetryAttempts&#39;: 0}}\n\n{&#39;EndpointConfigArn&#39;: &#39;arn:aws:sagemaker:us-west-2:360823276916:endpoint-config/news20-boto3&#39;, &#39;ResponseMetadata&#39;: {&#39;RequestId&#39;: &#39;6d4e9f8d-52d7-40c8-9f71-90e013801473&#39;, &#39;HTTPStatusCode&#39;: 200, &#39;HTTPHeaders&#39;: {&#39;x-amzn-requestid&#39;: &#39;6d4e9f8d-52d7-40c8-9f71-90e013801473&#39;, &#39;content-type&#39;: &#39;application/x-amz-json-1.1&#39;, &#39;content-length&#39;: &#39;93&#39;, &#39;date&#39;: &#39;Fri, 03 Apr 2020 06:36:19 GMT&#39;}, &#39;RetryAttempts&#39;: 0}}\n\n{&#39;EndpointArn&#39;: &#39;arn:aws:sagemaker:us-west-2:360823276916:endpoint/news20-boto3&#39;, &#39;ResponseMetadata&#39;: {&#39;RequestId&#39;: &#39;915c3d41-69ce-4a1f-bce9-1192cb2c07c7&#39;, &#39;HTTPStatusCode&#39;: 200, &#39;HTTPHeaders&#39;: {&#39;x-amzn-requestid&#39;: &#39;915c3d41-69ce-4a1f-bce9-1192cb2c07c7&#39;, &#39;content-type&#39;: &#39;application/x-amz-json-1.1&#39;, &#39;content-length&#39;: &#39;80&#39;, &#39;date&#39;: &#39;Fri, 03 Apr 2020 06:36:19 GMT&#39;}, &#39;RetryAttempts&#39;: 0}}\n\nCreating\nCreating\nCreating\nCreating\nCreating\nCreating\nInService\n</div>"]}}],"execution_count":25},{"cell_type":"markdown","source":["###### Concise deployment using `mlflow`"],"metadata":{}},{"cell_type":"code","source":["# mlflow integrated sagemaker deployment, API: https://www.mlflow.org/docs/latest/python_api/mlflow.sagemaker.html#mlflow.sagemaker.deploy\n# this only applies to the case where there is only one model, for model composition and pre/post-processing,  check future work in the last part\n\ntimestamp_prefix = time.strftime(\"%Y%m%d-%H%M\", time.gmtime())\napp_name = \"news20-mlflow-\"+timestamp_prefix\n\nmfs.deploy(app_name=app_name\n           , model_uri=model_uri\n           , region_name=region\n           , mode=\"create\"\n           , flavor='mleap'\n           , execution_role_arn=arn\n           , image_url=image_url\n           , instance_type='ml.m5.large'\n          )\nwhile True:\n    endpoint_status = sgm.describe_endpoint(EndpointName=app_name)['EndpointStatus']\n    print(endpoint_status)\n    if endpoint_status in ('OutOfService','InService','Failed'):\n        break\n    time.sleep(60)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">InService\n</div>"]}}],"execution_count":27},{"cell_type":"code","source":["# ============================================================\n# check the Endpoint status\n\ndef check_status(app_name):\n  sage_client = boto3.client('sagemaker', region_name=region)\n  endpoint_description = sage_client.describe_endpoint(EndpointName=app_name)\n  endpoint_status = endpoint_description[\"EndpointStatus\"]\n  return endpoint_status\n\nprint(f\"Application status is: {check_status(app_name)}\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Application status is: InService\n</div>"]}}],"execution_count":28},{"cell_type":"markdown","source":["##### Query the model"],"metadata":{}},{"cell_type":"code","source":["print(json.dumps(input_json_records))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">&#34;[{\\&#34;text\\&#34;:\\&#34;From: Ravi Konchigeri &lt;mongoose@leland.stanford.edu&gt; Subject: Re: LCIII problems X-Xxmessage-Id: &lt;A7F4A76B690100ED@kimball-pc-316.stanford.edu&gt; X-Xxdate: Fri, 16 Apr 93 02:11:55 GMT Organization: Stanford University X-Useragent: Nuntius v1.1.1d17 Lines: 24  In article &lt;1qmgjk$ao5@menudo.uh.edu&gt; , sunnyt@coding.bchs.uh.edu writes: &gt; Its not a good idea to have a horizontally formatted hard disk in a   &gt;vertical position.  If the drive is formatted in a horizontal position, it can   &gt;not completely compensate for the gravitational pull in a vertical position.    &gt;I&#39;m not saying that your hard disk will fail tomorrow or 6 months from now, but   &gt;why take that chance?  If you want more detailed info on the problem, please    I think the other replies sum up the fact that you can place a hard drive on its side.  The point is this will only be sure to work on the &#39;new&#39; drives, namely 1\\\\/3 ht LPS drives that have a smaller platter and are also more stable.  Why should I take the chance?  Because I&#39;ve been running a Maxtor 1\\\\/3 ht 120 LPS on both its side and flat for about a year and I&#39;ve had no problems with it.  Period.  Like I always say, NEVER trust the manufacturer.   \\\\\\&#34;Just like everything else in life, the right lane ends in half a mile.\\\\\\&#34;  Ravi Konchigeri. mongoose@leland.stanford.edu \\&#34;}]&#34;\n</div>"]}}],"execution_count":30},{"cell_type":"code","source":["# ============================================================\n# prepare the querying data (aka the 'payload')\n\n# Enable Arrow-based columnar data transfers for faster [Spark DF --> Pandas DF] transformation\nspark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\nspark.conf.set(\"spark.sql.execution.arrow.fallback.enabled\", \"true\")\n\n# Note that if using mlflow, the deployed MLeap models only process JSON-serialized Pandas dataframes with the split orientation:\n# if using boto3, the deployed models only accepts 'text/csv' ContentType of request\n\nquery_df = test_with_label.drop('topic').sample(False, 0.002, 41).limit(1).toPandas()\ninput_json_records = query_df.to_json(orient='records')\ninput_json_split = query_df.to_json(orient='split')\ninput_csv = query_df.to_csv()\ninput_csv"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[35]: &#39;,text\\n0,&#34;From: Ravi Konchigeri &lt;mongoose@leland.stanford.edu&gt; Subject: Re: LCIII problems X-Xxmessage-Id: &lt;A7F4A76B690100ED@kimball-pc-316.stanford.edu&gt; X-Xxdate: Fri, 16 Apr 93 02:11:55 GMT Organization: Stanford University X-Useragent: Nuntius v1.1.1d17 Lines: 24  In article &lt;1qmgjk$ao5@menudo.uh.edu&gt; , sunnyt@coding.bchs.uh.edu writes: &gt; Its not a good idea to have a horizontally formatted hard disk in a   &gt;vertical position.  If the drive is formatted in a horizontal position, it can   &gt;not completely compensate for the gravitational pull in a vertical position.    &gt;I\\&#39;m not saying that your hard disk will fail tomorrow or 6 months from now, but   &gt;why take that chance?  If you want more detailed info on the problem, please    I think the other replies sum up the fact that you can place a hard drive on its side.  The point is this will only be sure to work on the \\&#39;new\\&#39; drives, namely 1/3 ht LPS drives that have a smaller platter and are also more stable.  Why should I take the chance?  Because I\\&#39;ve been running a Maxtor 1/3 ht 120 LPS on both its side and flat for about a year and I\\&#39;ve had no problems with it.  Period.  Like I always say, NEVER trust the manufacturer.   &#34;&#34;Just like everything else in life, the right lane ends in half a mile.&#34;&#34;  Ravi Konchigeri. mongoose@leland.stanford.edu &#34;\\n&#39;</div>"]}}],"execution_count":31},{"cell_type":"code","source":["# ============================================================\n# query the model for boto3\ndef query_endpoint_boto3(app_name, input):\n  client = boto3.session.Session().client(\"sagemaker-runtime\", region)\n  \n  response = client.invoke_endpoint(\n      EndpointName=app_name\n      , Body=input\n      , ContentType='text/csv'\n    # or pass in schema together with data by specifying: \n    # Body='{\"schema\":{\"input\": [{\"name\": \"text\", \"type\": \"string\"}], \"output\": {\"type\": \"double\", \"name\": \"prediction\"}}, \"data\":['payload'}',\n    # ContentType='application/json'\n  )\n    \nprint('Our result for this payload is: {}'.format(response['Body'].read().decode('ascii')))\n  preds = response['Body'].read().decode(\"ascii\")\n  print(f\"Received response: {preds}\")\n  return preds\n\nprint(\"Sending batch prediction request with input csv format...\")\n# ============================================================\n# Evaluate the input by posting it to the deployed model\nprediction = query_endpoint_boto3(app_name=app_name, input=input_csv)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Sending batch prediction request with input csv format...\nReceived response: misc.forsale\n</div>"]}}],"execution_count":32},{"cell_type":"code","source":["# ============================================================\n# query the model for mlflow\ndef query_endpoint_mlflow(app_name, input):\n  client = boto3.session.Session().client(\"sagemaker-runtime\", region)\n  \n  response = client.invoke_endpoint(\n      EndpointName=app_name\n      , Body=input\n      , ContentType='application/json'\n  )\n  preds = response['Body'].read().decode(\"ascii\")\n  print(f\"Received response: {preds}\")\n  return preds\n\nprint(\"Sending batch prediction request with input dataframe json...\")\n\n# ============================================================\n# Evaluate the input by posting it to the deployed model\n\nprediction = query_endpoint_mlflow(app_name=app_name, input=input_json_split)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Sending batch prediction request with input dataframe json...\nReceived response: [0.0]\n</div>"]}}],"execution_count":33},{"cell_type":"markdown","source":["##### clean up"],"metadata":{}},{"cell_type":"code","source":["# Environment cleanup\n\n# for boto3:\ndef clean_up(app_name):\n  client = boto3.session.Session().client(\"sagemaker\", region)\n  print('Deleting SageMaker endpoint...')\n  print(client.delete_endpoint(EndpointName=app_name\n                                 )\n       )\n\n  print('Deleting SageMaker endpoint config...')\n  print(client.delete_endpoint_config(EndpointConfigName=app_name\n                                        )\n       )\n\n  print('Deleting SageMaker model...')\n  print(client.delete_model(ModelName=app_name\n                              )\n       )\nclean_up(app_name)\n\n# # for mlflow\n# def clean_up(app_name):\n#   mfs.delete(app_name=app_name, \n#              region_name=region, \n#              archive=False # this will clean associated Models, Endpoints and Endpoint Configs\n#             )\n# clean_up(app_name)\n\n# to check if the clean_up is successful:\ndef get_active_endpoints(app_name):\n  sage_client = boto3.client('sagemaker', region_name=region)\n  app_endpoints = sage_client.list_endpoints(NameContains=app_name)[\"Endpoints\"]\n  return list(filter(lambda en : en == app_name, [str(endpoint[\"EndpointName\"]) for endpoint in app_endpoints]))\n\ntime.sleep(5)\nprint(\"The following endpoints exist for the `{an}` application: {eps}\".format(an=app_name, eps=get_active_endpoints(app_name)))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Deleting SageMaker endpoint...\n{&#39;ResponseMetadata&#39;: {&#39;RequestId&#39;: &#39;96c5a770-ce42-4f92-b5dc-62c6c5375a30&#39;, &#39;HTTPStatusCode&#39;: 200, &#39;HTTPHeaders&#39;: {&#39;x-amzn-requestid&#39;: &#39;96c5a770-ce42-4f92-b5dc-62c6c5375a30&#39;, &#39;content-type&#39;: &#39;application/x-amz-json-1.1&#39;, &#39;content-length&#39;: &#39;0&#39;, &#39;date&#39;: &#39;Fri, 03 Apr 2020 06:58:41 GMT&#39;}, &#39;RetryAttempts&#39;: 0}}\nDeleting SageMaker endpoint config...\n{&#39;ResponseMetadata&#39;: {&#39;RequestId&#39;: &#39;a1aa0100-2539-417a-94e7-6f8f2dcb1460&#39;, &#39;HTTPStatusCode&#39;: 200, &#39;HTTPHeaders&#39;: {&#39;x-amzn-requestid&#39;: &#39;a1aa0100-2539-417a-94e7-6f8f2dcb1460&#39;, &#39;content-type&#39;: &#39;application/x-amz-json-1.1&#39;, &#39;content-length&#39;: &#39;0&#39;, &#39;date&#39;: &#39;Fri, 03 Apr 2020 06:58:41 GMT&#39;}, &#39;RetryAttempts&#39;: 0}}\nDeleting SageMaker model...\n{&#39;ResponseMetadata&#39;: {&#39;RequestId&#39;: &#39;193f54e3-afef-4356-8204-57c40a926baf&#39;, &#39;HTTPStatusCode&#39;: 200, &#39;HTTPHeaders&#39;: {&#39;x-amzn-requestid&#39;: &#39;193f54e3-afef-4356-8204-57c40a926baf&#39;, &#39;content-type&#39;: &#39;application/x-amz-json-1.1&#39;, &#39;content-length&#39;: &#39;0&#39;, &#39;date&#39;: &#39;Fri, 03 Apr 2020 06:58:41 GMT&#39;}, &#39;RetryAttempts&#39;: 0}}\nThe following endpoints exist for the `news20-boto3` application: []\n</div>"]}}],"execution_count":35},{"cell_type":"markdown","source":["#### Future work / TODO list:\n- sustitute the Spark Model with AWS SageMaker built-in model ( add pre-processing and training algorithms to separate containers)\n>The MLflow Docker container for serving on SageMaker runs a Flask server internally that accepts JSON-formatted or CSV-formatted Pandas DataFrames. That's how the current content types and serving input schemas are enforced. Regarding model composition and pre/postprocessing, you can use custom pyfunc APIs to package multiple models together along with custom logic: https://mlflow.org/docs/latest/models.html#custom-python-models\n- deploy pipeline for batch inference"],"metadata":{}}],"metadata":{"name":"Inference pipeline deployment to AWS SageMaker","notebookId":3490305292750161},"nbformat":4,"nbformat_minor":0}
